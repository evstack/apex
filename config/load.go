package config

import (
	"bytes"
	"errors"
	"fmt"
	"os"

	"github.com/evstack/apex/pkg/types"
	"gopkg.in/yaml.v3"
)

// Generate writes a default config file with comments to the given path.
// Returns an error if the file already exists.
func Generate(path string) error {
	_, err := os.Stat(path)
	if err == nil {
		return fmt.Errorf("config file %q already exists", path)
	}
	if !errors.Is(err, os.ErrNotExist) {
		return fmt.Errorf("checking config path: %w", err)
	}
	return os.WriteFile(path, []byte(defaultConfigYAML), 0o644)
}

// defaultConfigYAML is the template written by Generate.
// Keep values in sync with DefaultConfig() â€” Load() decodes YAML on top of
// DefaultConfig(), so divergence means `apex init` output won't match runtime defaults.
const defaultConfigYAML = `# Apex configuration
# Generated by: apex init

data_source:
  # Data source type: "node" (Celestia DA node) or "app" (celestia-app CometBFT RPC)
  type: "node"

  # Celestia DA node RPC endpoint (required when type: "node")
  celestia_node_url: "http://localhost:26658"

  # Celestia-app CometBFT RPC endpoint (required when type: "app")
  # celestia_app_url: "http://localhost:26657"

  # Auth token: set via APEX_AUTH_TOKEN env var (not read from this file).

  # Namespaces to index (hex-encoded, 29 bytes = 58 hex chars each).
  namespaces: []

storage:
  # Storage backend: "sqlite" (default) or "s3"
  type: "sqlite"

  # Path to the SQLite database file (used when type: "sqlite")
  db_path: "apex.db"

  # S3-compatible object store settings (used when type: "s3")
  # s3:
  #   bucket: "my-apex-bucket"
  #   prefix: "indexer"
  #   region: "us-east-1"
  #   endpoint: ""          # custom endpoint for MinIO, R2, etc.
  #   chunk_size: 64        # heights per S3 object

rpc:
  # Address for the JSON-RPC API server (HTTP/WebSocket)
  listen_addr: ":8080"
  # Address for the gRPC API server
  grpc_listen_addr: ":9090"

sync:
  # Height to start syncing from (0 = genesis)
  start_height: 0
  # Number of headers per backfill batch
  batch_size: 64
  # Number of concurrent fetch workers
  concurrency: 4

subscription:
  # Event buffer size per subscriber (for API subscriptions)
  buffer_size: 64

metrics:
  # Enable Prometheus metrics endpoint
  enabled: true
  # Address for the metrics server
  listen_addr: ":9091"

profiling:
  # Enable pprof endpoints (/debug/pprof/*)
  enabled: false
  # Bind address for profiling HTTP server (prefer loopback)
  listen_addr: "127.0.0.1:6061"

log:
  # Log level: trace, debug, info, warn, error, fatal, panic
  level: "info"
  # Log format: json or console
  format: "json"
`

var validLogLevels = map[string]bool{
	"trace": true, "debug": true, "info": true,
	"warn": true, "error": true, "fatal": true, "panic": true,
}

// Load reads and validates a YAML config file at the given path.
// Environment variable APEX_AUTH_TOKEN overrides data_source.auth_token.
func Load(path string) (*Config, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("reading config: %w", err)
	}

	cfg := DefaultConfig()

	dec := yaml.NewDecoder(bytes.NewReader(data))
	dec.KnownFields(true)
	if err := dec.Decode(&cfg); err != nil {
		return nil, fmt.Errorf("parsing config: %w", err)
	}

	// Env var override.
	if token := os.Getenv("APEX_AUTH_TOKEN"); token != "" {
		cfg.DataSource.AuthToken = token
	}

	if err := validate(&cfg); err != nil {
		return nil, fmt.Errorf("validating config: %w", err)
	}

	return &cfg, nil
}

func validateDataSource(ds *DataSourceConfig) error {
	switch ds.Type {
	case "node", "":
		if ds.CelestiaNodeURL == "" {
			return fmt.Errorf("data_source.celestia_node_url is required for type \"node\"")
		}
	case "app":
		if ds.CelestiaAppURL == "" {
			return fmt.Errorf("data_source.celestia_app_url is required for type \"app\"")
		}
	default:
		return fmt.Errorf("data_source.type %q is invalid; must be \"node\" or \"app\"", ds.Type)
	}
	for _, ns := range ds.Namespaces {
		if _, err := types.NamespaceFromHex(ns); err != nil {
			return fmt.Errorf("invalid namespace %q: %w", ns, err)
		}
	}
	return nil
}

func validateStorage(s *StorageConfig) error {
	switch s.Type {
	case "s3":
		if s.S3 == nil {
			return fmt.Errorf("storage.s3 is required when storage.type is \"s3\"")
		}
		if s.S3.Bucket == "" {
			return fmt.Errorf("storage.s3.bucket is required")
		}
		if s.S3.Region == "" && s.S3.Endpoint == "" {
			return fmt.Errorf("storage.s3.region is required (unless endpoint is set)")
		}
		if s.S3.ChunkSize == 0 {
			s.S3.ChunkSize = 64
		}
		if s.S3.ChunkSize < 0 {
			return fmt.Errorf("storage.s3.chunk_size must be positive")
		}
	case "sqlite", "":
		if s.DBPath == "" {
			return fmt.Errorf("storage.db_path is required")
		}
	default:
		return fmt.Errorf("storage.type %q is invalid; must be \"sqlite\" or \"s3\"", s.Type)
	}
	return nil
}

func validate(cfg *Config) error {
	if err := validateDataSource(&cfg.DataSource); err != nil {
		return err
	}
	if err := validateStorage(&cfg.Storage); err != nil {
		return err
	}
	if cfg.RPC.ListenAddr == "" {
		return fmt.Errorf("rpc.listen_addr is required")
	}
	if cfg.RPC.GRPCListenAddr == "" {
		return fmt.Errorf("rpc.grpc_listen_addr is required")
	}
	if cfg.Sync.BatchSize <= 0 {
		return fmt.Errorf("sync.batch_size must be positive")
	}
	if cfg.Sync.Concurrency <= 0 {
		return fmt.Errorf("sync.concurrency must be positive")
	}
	if cfg.Subscription.BufferSize <= 0 {
		return fmt.Errorf("subscription.buffer_size must be positive")
	}
	if cfg.Metrics.Enabled && cfg.Metrics.ListenAddr == "" {
		return fmt.Errorf("metrics.listen_addr is required when metrics are enabled")
	}
	if cfg.Profiling.Enabled && cfg.Profiling.ListenAddr == "" {
		return fmt.Errorf("profiling.listen_addr is required when profiling is enabled")
	}
	if !validLogLevels[cfg.Log.Level] {
		return fmt.Errorf("log.level %q is invalid; must be one of trace/debug/info/warn/error/fatal/panic", cfg.Log.Level)
	}
	if cfg.Log.Format != "json" && cfg.Log.Format != "console" {
		return fmt.Errorf("log.format %q is invalid; must be json or console", cfg.Log.Format)
	}

	return nil
}
